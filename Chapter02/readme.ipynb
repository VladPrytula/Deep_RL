{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cwiki.apache.org/confluence/download/attachments/69406797/inProgress.gif?version=1&modificationDate=1493416081000&api=v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first steps of Reinforcement learning journey (from Cartpole to Doom). \n",
    "\n",
    "# Section 1. Cartpole\n",
    "\n",
    "### Prerequisites\n",
    "* Python and PyTorch\n",
    "* Open AI gym\n",
    "* openCV\n",
    "\n",
    "As a first excersise we will try to solve the \"hello world\" of RL problems, namely the \"cartPole\" problem. The original description is here [OpenAI Gym Cartpole](https://gym.openai.com/envs/CartPole-v0/).\n",
    "\n",
    "From the RL point of view the two main entities are:\n",
    "\n",
    "* Agent\n",
    "* * A entity that takes an action (i.e. the role is active). In our case that will be some piece of code that descides what action to take based on the _observation_ obtained from the environment\n",
    "* Environment\n",
    "* * Certain representation of the world, which provide agent the observations and reward (can be heavily delayed in time). Environment can be deterministic (with respect to (state,action) pair) or stochastic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toy example of the Environment and the Agent in Python\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def perform_action(self, env):\n",
    "        actions = env.get_actions()\n",
    "        action = random.choice(actions)\n",
    "        reward, observatoin, is_done = env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.time_to_live = 10\n",
    "        self.is_done = False\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward, observation, is_done = None, None, False\n",
    "        if self.is_done:\n",
    "            reward = 1\n",
    "            observation = None\n",
    "            is_done = True\n",
    "        else:\n",
    "            reward = 1\n",
    "            observation = [0,1, 0.2]\n",
    "            self.time_to_live -= 1\n",
    "\n",
    "        return reward, observation, is_done\n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.time_to_live == 0\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        return [1, -1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.time_to_live = 10\n",
    "        self.is_done = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at a real example that is provided by OpenAI gym - CartPol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.00114557, -0.1786382 , -0.00098355,  0.28465194]), 1.0, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_agent(env):\n",
    "    total_reward: float = 0.0\n",
    "    total_steps: int = 0\n",
    "    obs = env.reset() # start the episode\n",
    "\n",
    "    while True:\n",
    "        sample_action = env.action_space.sample()\n",
    "        obs, reward, is_done, info = env.step(sample_action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode contained %i steps, reward obtained is %.2f\" % (total_steps, reward))\n",
    "\n",
    "    pass\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     env = gym.make(\"CartPole-v0\")\n",
    "#     # wrap our environment in a monitor\n",
    "#     env = gym.wrappers.Monitor(env, \"recording\")\n",
    "#     #run the random agent\n",
    "#     run_random_agent(env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"recording\")\n",
    "# if running on mac we have to have ffmpg\n",
    "run_random_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: there is a certain issue that one might run into while trying to record the episode that. On linux that can be solved using the Xvbf\n",
    "> `xvbf-run -s \"--screen 0 640x480x24\" python random_agent_cartpole.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entoropy method for CartPole invironment\n",
    "References \n",
    "\n",
    "[ P. T. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. A tutorial on the cross-entropy method. Annals of Operations Research, 134(1):19–67, 2005.](https://people.smp.uq.edu.au/DirkKroese/ps/aortut.pdf)\n",
    "\n",
    "[Dirk P. Cross-Entropy method. Kroese School of Mathematics and Physics](https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf)\n",
    "\n",
    "The CE method involves an iterative procedure where each iteration can be broken\n",
    "down into two phases:\n",
    "1. Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism.\n",
    "2. Update the parameters of the random mechanism based on the data to produce a “better” sample in the next iteration.\n",
    "\n",
    "\n",
    "> The following is mostly from here: [The Cross-Entropy Method for Estimation Dirk P. Kroese1, Reuven Y. Rubinstein2, and Peter W. Glynn](https://web.stanford.edu/~glynn/papers/2013/KroeseRubinsteinG13.pdf)\n",
    "\n",
    "As it is stated in the above referenece article : _\"The CE method was introduced by Rubinstein (1999, 2001), extending earlier\n",
    "work on variance minimization (Rubinstein, 1997). Originally, the CE method\n",
    "was developed as a means of computing rare-event probabilities; that is, very\n",
    "small probabilities—say less than 10−4. Naive Monte Carlo estimation of such a probability requires a large simulation effort, inversely proportional to the\n",
    "magnitude of the rare-event probability. The CE method is based on two ideas.\n",
    "The first idea is to estimate the probability of interest by gradually changing the\n",
    "sampling distribution, from the original to a distribution for which the rare event is\n",
    "much more likely to happen. To remove the estimation bias, importance sampling is\n",
    "used. The second idea is to use the CE distance to construct the sequence of sampling\n",
    "distributions. This significantly simplifies the numerical computation at each step,\n",
    "and provides fast and efficient algorithms that are easy to implement by practitioners\"_\n",
    "\n",
    "#### Problem setting:\n",
    "Genearlly we want to get the estimation of the expectation\n",
    "$$\n",
    "l = \\mathbb{E}_f[H(X)] = \\int H(x) f(x) dx,\n",
    "$$\n",
    "where $H$ is some real-valued function and $f$ is the probability density fucntion of a random variable $X$\n",
    "\n",
    "In the RL setting $H(x)$ is a reward value obtained by some policy $x$ (**TODO: define what policy is**) and $f(x)$ is a distribution of all possible policies. We don't want to maximize our reward by searching all possible policies, instead we want to find a way to approximate $f(x)H(x)$ by some $q(x)$, iteratively minimizing the distance between them.\n",
    "\n",
    "*Definition: Policy*:\n",
    "\n",
    "In our case of the CartPole the $H(x)$ can be replaced by an indicator function when the total reward for the episode is higher than certain threshold.**TODO: why can we do it?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
